{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianedp/ExpertSystem/blob/main/%5BONLINE%5D_Exemplo_2_Agente_de_di%C3%A1logo_baseado_em_regras_e_dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIMZgaR3IqM"
      },
      "source": [
        "# Exemplo 2 - Agente de diálogo híbrido (baseado em regras e dados)\n",
        "## Tecnólogo em Inteligência Artificial Aplicada - Agentes Conversacionais\n",
        "Neste notebook iremos construir um agente de diálogo que trará ocorrências sobre determinado tema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDw6X5Dq3h_Z"
      },
      "source": [
        "### Qual o contexto de nosso agente?\n",
        "\n",
        "Iremos desenvolver um agente de diálogo de question answering, que baseado em um corpus de texto sobre um assunto, trará as informações mais relevantes de acordo com a consulta do usuário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlqXcfuAPbQ"
      },
      "source": [
        "### Quais ferramentas e técnicas iremos utilizar?\n",
        "\n",
        "*   **NLTK** - O mais famoso toolkit de Processamento de Linguagem Natural em Python.\n",
        "*   **Expressões Regulares** - o pacote de regex do Python será utilizado para otimizar a busca de padrões.\n",
        "*   **urllib e BeautifulSoup** - Bibliotecas para obter dados de páginas HTML.\n",
        "*   **scikit-learn** - Pacote com funcionalidades de manipulaçã de dados e Machine Learning, vamos utilizar TF-IDF e Similaridade de cosseno.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHKrJBjdBqw1"
      },
      "source": [
        "### Construindo o agente de diálogo\n",
        "Nosso agente vai operar da seguinte maneira:\n",
        "\n",
        "1.   Recebe **entrada** do usuário\n",
        "2.   **Pré-processa** a entrada do usuário\n",
        "3.   Calcula a **similaridade** entre a entrada e as sentenças do corpus\n",
        "4.   Obtém a sentença **mais similar do corpus**\n",
        "5.   Mostra-a como **resposta** ao usuário\n",
        "\n",
        "Anteriormente a estas etapas, iremos criar nosso corpus ao obter dados da Wikipedia, automaticamente, então vamos lá!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb2Dp_OFJAyO"
      },
      "source": [
        "#### Importando as bibliotecas\n",
        "Vamos importar o pacote de expressões regulares do Python e também o acesso ao WordNet dado pelo NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMVzxB8RB-F"
      },
      "source": [
        "#### Construindo o corpus\n",
        "Vamos fazer um *web-scraping* para obter os dados automaticamente da wikipedia. Este processo precisa ser executado apenas uma vez, e o arquivo salvo em forma de texto na máquina."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download dos recursos do NLTK\n",
        "nltk.download('punkt')      # tokenização\n",
        "nltk.download('rslp')       # stemmer em português\n",
        "nltk.download('stopwords')  # lista de stopwords\n",
        "\n",
        "# Ignorar alguns avisos desnecessários\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dGE5UxRBKdG",
        "outputId": "f6fdaf2a-e5cb-491e-d9c8-5886753bacc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAXpxbwT5IB"
      },
      "source": [
        "##### Pré-processando o corpus\n",
        "Como você pode ver no trecho acima, precisamos remover caracteres especiais do texto, além de dividí-lo em sentenças válidas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui eu busquei todos os parágrafos do artigo\n",
        "paragrafos = html_processado.find_all('p')\n",
        "\n",
        "texto = ''\n",
        "\n",
        "# Eu percorri os parágrafos e juntei os textos\n",
        "for p in paragrafos:\n",
        "    texto += p.text\n",
        "\n",
        "# Aqui eu também peguei os dados da tabela lateral (infobox)\n",
        "infobox = html_processado.find('table', {'class':'infobox'})\n",
        "if infobox:\n",
        "    for linha in infobox.find_all('tr'):\n",
        "        dados = linha.get_text(\" \", strip=True)\n",
        "        texto += \" \" + dados\n",
        "\n",
        "# Agora eu adicionei manualmente algumas frases curtas,\n",
        "# pra garantir que o bot saiba responder perguntas básicas.\n",
        "texto += \" O presidente do Brasil é Luiz Inácio Lula da Silva. \"\n",
        "texto += \" A população do Brasil é de aproximadamente 212 milhões de habitantes. \"\n",
        "texto += \" O Brasil tem 26 estados e o Distrito Federal. \"\n",
        "texto += \" O Brasil fica na América do Sul e faz fronteira com quase todos os países, exceto Chile e Equador. \"\n",
        "\n",
        "# Normalizei tudo para minúsculas\n",
        "texto = texto.lower()\n",
        "\n",
        "# Visualizando só o início pra conferir\n",
        "texto[0:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Gu8ztMWzB7O4",
        "outputId": "528c75b4-7099-4c13-9423-face74b2cc45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbrasil (localmente\\xa0[bɾaˈziw][c]), oficialmente república federativa do brasil (escutarⓘ),[12] é o maior país da américa do sul e da região da américa latina, sendo o quinto maior do mundo em área territorial (equivalente a 47,3% do território sul-americano), com cerca de 8,5 milhões de quilômetros quadrados,[5][13][14] e o sétimo em população[15][16] (com 212 milhões de habitantes, em julho de 2024).[17] é o único país na américa onde se fala majoritariamente a língua portuguesa e o maior país lusófono do planeta,[18] além de ser uma das nações mais multiculturais e etnicamente diversas, em decorrência da forte imigração oriunda de variados locais do mundo. sua atual constituição, promulgada em 1988, concebe o brasil como uma república federativa presidencialista,[12] formada pela união de 26 estados, do distrito federal e dos 5\\xa0571 municípios.[12][19][nota 1]\\nbanhado pelo oceano atlântico, o brasil tem um litoral de 7\\xa0491\\xa0km[18] e faz fronteira com todos os outros países sul-american'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui eu estou limpando o texto, tirando as referências tipo [1], [2], etc.\n",
        "texto = re.sub(r'\\[[0-9]*\\]', ' ', texto)\n",
        "\n",
        "# Também deixo só um espaço onde tiver muitos espaços seguidos\n",
        "texto = re.sub(r'\\s+', ' ', texto)\n",
        "\n",
        "# Aqui eu mostro um pedaço do texto só pra conferir\n",
        "texto[0:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "b6ShhTpPCcSY",
        "outputId": "dde16af3-3c16-4a5a-9e0c-3e9b5ff29f5b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' brasil (localmente [bɾaˈziw][c]), oficialmente república federativa do brasil (escutarⓘ), é o maior país da américa do sul e da região da américa latina, sendo o quinto maior do mundo em área territorial (equivalente a 47,3% do território sul-americano), com cerca de 8,5 milhões de quilômetros quadrados, e o sétimo em população (com 212 milhões de habitantes, em julho de 2024). é o único país na américa onde se fala majoritariamente a língua portuguesa e o maior país lusófono do planeta, além de ser uma das nações mais multiculturais e etnicamente diversas, em decorrência da forte imigração oriunda de variados locais do mundo. sua atual constituição, promulgada em 1988, concebe o brasil como uma república federativa presidencialista, formada pela união de 26 estados, do distrito federal e dos 5 571 municípios. [nota 1] banhado pelo oceano atlântico, o brasil tem um litoral de 7 491 km e faz fronteira com todos os outros países sul-americanos, exceto chile e equador, sendo limitado a n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIjyC9OJC7e2",
        "outputId": "5de5daa4-8758-496a-f7eb-acbe1ba12cd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui eu quebrei o texto em sentenças\n",
        "sentencas = nltk.sent_tokenize(texto, language='portuguese')\n",
        "\n",
        "# Também quebrei em palavras\n",
        "palavras = nltk.word_tokenize(texto, language='portuguese')\n",
        "\n",
        "# Mostro algumas sentenças só pra conferir\n",
        "sentencas[10:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2f1xOZYCmEu",
        "outputId": "db7015a0-4537-4a40-f5a5-304fc6a8edec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['em 1815, o brasil se torna parte de um reino unido com portugal.',\n",
              " 'dom pedro i, o primeiro imperador, proclamou a independência política do país em 1822. inicialmente independente como um império, período no qual foi uma monarquia constitucional parlamentarista, o brasil tornou-se uma república em 1889, em razão de um golpe militar chefiado pelo marechal deodoro da fonseca (o primeiro presidente), embora uma legislatura bicameral, agora chamada de congresso nacional, já existisse desde a ratificação da primeira constituição, em 1824. desde o início do período republicano, a governança democrática foi interrompida por longos períodos de regimes autoritários, até um governo civil e eleito democraticamente assumir o poder em 1985, com o fim da ditadura militar.',\n",
              " 'como potência regional e média, a nação tem reconhecimento e influência internacional, sendo que também é classificada como uma potência global emergente e como uma potencial superpotência por vários analistas.',\n",
              " 'o pib nominal brasileiro é o nono maior do mundo e o oitavo por paridade do poder de compra (ppc), sendo, em ambos, o maior da américa latina e do hemisfério sul.',\n",
              " 'o país é um dos principais celeiros do planeta, sendo o maior produtor de café dos últimos 150 anos, além de ser classificado como uma economia de renda média-alta pelo banco mundial e como um país recentemente industrializado, que detém a maior parcela de riqueza global e a ecomomia mais complexa da américa do sul.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nR3LBTVmy0"
      },
      "source": [
        "#### Funções de pré-processamento de entrada do usuário\n",
        "Vamos criar funções para pré-processar as entradas do usuário, vamos retirar pontuações e usar Stemming nos textos, para que palavras similares sejam processadas de maneira igual pelo algoritmo (e.g., pedra e pedregulho teriam a mesma forma léxica)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYC2-NpmDd7L",
        "outputId": "ac0bbcb0-4787-4fd6-ca42-eed00cf2ddd6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjMnCjqfVm6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f069a8-9772-48c1-85d2-dd7445896f91"
      },
      "source": [
        "# Aqui eu criei uma função que faz o Stemming, ou seja,\n",
        "# ela pega as palavras e reduz para a raiz delas.\n",
        "def stemming(tokens):\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    novo_texto = []\n",
        "    for token in tokens:\n",
        "        novo_texto.append(stemmer.stem(token.lower()))\n",
        "    return novo_texto\n",
        "\n",
        "# Aqui eu monto um dicionário para remover pontuações\n",
        "removePontuacao = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "# Essa função faz o pré-processamento completo:\n",
        "# - coloca em minúsculas\n",
        "# - tira acento\n",
        "# - remove pontuação\n",
        "# - tira stopwords\n",
        "# - aplica stemming\n",
        "import unidecode\n",
        "stop_words = set(stopwords.words(\"portuguese\"))\n",
        "\n",
        "def preprocessa(documento):\n",
        "    # normalizo: minúsculas e sem acento\n",
        "    doc = unidecode.unidecode(documento.lower())\n",
        "    # tiro a pontuação\n",
        "    doc = doc.translate(removePontuacao)\n",
        "    # quebro em tokens\n",
        "    tokens = nltk.word_tokenize(doc, language=\"portuguese\")\n",
        "    # tiro stopwords e aplico stemming\n",
        "    tokens = [n for n in stemming(tokens) if n not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Testando como fica depois do pré-processamento\n",
        "preprocessa(\"Olá meu nome é Lucas, eu moro no Brasil, e você?\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ola', 'nom', 'luc', 'mor', 'brasil', 'voc']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUL4YvxMYMVc"
      },
      "source": [
        "#### Resposta à saudações\n",
        "Mesmo que estejamos construindo um sistema de diálogo (baseado em tarefas), é normal que o usuário inicie conversas com saudações ao agente. Portanto, iremos desenvolver rapidamente uma função (i.e., regras) para lidar especialmente com esta situação.\n",
        "Vamos criar algumas respostas possíveis, e sempre vamos escolher aleatóriamente uma delas, para evitar que nosso agente fique repetitivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJNLddZgYMeg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a93be5fa-d15c-439c-dcb8-42e83f5d1a7a"
      },
      "source": [
        "# Aqui eu defini algumas formas que o usuário pode usar para cumprimentar o bot\n",
        "saudacoes_entrada = (\"olá\", \"bom dia\", \"boa tarde\", \"boa noite\", \"oi\", \"como vai\", \"e aí\")\n",
        "\n",
        "# E aqui eu fiz uma listinha de respostas possíveis do bot\n",
        "saudacoes_respostas = [\n",
        "    \"olá\",\n",
        "    \"olá, espero que esteja tudo bem contigo\",\n",
        "    \"oi\",\n",
        "    \"oie\",\n",
        "    \"seja bem-vindo, em que posso te ajudar?\"\n",
        "]\n",
        "\n",
        "# Essa função verifica se a entrada do usuário foi uma saudação\n",
        "# Se for, eu retorno uma resposta aleatória da lista acima\n",
        "def geradorsaudacoes(saudacao):\n",
        "    for token in saudacao.split():\n",
        "        if token.lower() in saudacoes_entrada:\n",
        "            return random.choice(saudacoes_respostas)\n",
        "\n",
        "# Testando várias vezes eu vou ver respostas diferentes\n",
        "geradorsaudacoes(\"Olá\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_gdo4tjZ3X4"
      },
      "source": [
        "#### Resposta à consultas do usuário\n",
        "Agora, teremos uma função para lidar com consultas do usuário. Onde iremos comparar a similaridade entre a entrada do usuário, com as sentenças do corpus. Caso encontremos, a sentença mais similar será mostrada como resposta."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Essa função vai gerar a resposta do bot\n",
        "def geradorrespostas(entradausuario):\n",
        "    resposta = ''\n",
        "\n",
        "    # Eu adiciono a pergunta do usuário junto com as sentenças do corpus\n",
        "    sentencas.append(entradausuario)\n",
        "\n",
        "    # Aqui eu crio a matriz TF-IDF usando minha função de pré-processamento\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=preprocessa, stop_words=stopwords.words('portuguese'))\n",
        "    all_word_vectors = word_vectorizer.fit_transform(sentencas)\n",
        "\n",
        "    # Calculo a similaridade de cosseno entre a entrada e as sentenças\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "\n",
        "    # Pego a sentença mais parecida (tirando a última, que é a própria pergunta)\n",
        "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    # Se a similaridade for zero ou muito baixa, eu aviso que não encontrei nada\n",
        "    if vector_matched < 0.3:  # aqui eu coloquei um limite de confiança\n",
        "        resposta = \"Me desculpe, não encontrei nada relevante sobre isso.\"\n",
        "    else:\n",
        "        resposta = sentencas[similar_sentence_number]\n",
        "\n",
        "    # Eu removo a pergunta do usuário da lista de sentenças para não poluir o corpus\n",
        "    sentencas.pop(-1)\n",
        "\n",
        "    return resposta"
      ],
      "metadata": {
        "id": "P7LZzJtsEVWG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VQILIt-bgJ8"
      },
      "source": [
        "#### Interagindo com o agente de diálogo\n",
        "Vamos definir um algoritmo que continue interagindo com o usuário até que ele decida finalizar.\n",
        "\n",
        "O resultado não é sempre o ideal, mas já cobre muitas possíveis perguntas. Se utilizássemos apenas regras de diálogo para responder perguntas sobre um tema, precisaríamos de centenas de regras para tal. Mas como baseamos nossas respostas em dados apenas uma regra que calcula similaridade com nosso corpus já é o suficiente.\n",
        "\n",
        "Faça perguntas como:\n",
        "*  *Qual o esporte mais popular no Brasil?*\n",
        "*  *Quais eventos esportivos o Brasil já organizou?*\n",
        "*  *Como é a cozinha brasileira?*\n",
        "*  *Onde são realizadas pesquisas tecnológicas no Brasil?*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMIT3KygbgTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be13ecb0-5495-42ac-8d94-ba15c449ba42"
      },
      "source": [
        "continue_dialogue = True\n",
        "print(\"Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre o Brasil.\")\n",
        "\n",
        "while continue_dialogue:\n",
        "    # Aqui eu pego a entrada do usuário\n",
        "    human_text = input(\"Você: \").lower()\n",
        "\n",
        "    # Se o usuário disser tchau, eu encerro\n",
        "    if human_text == \"tchau\":\n",
        "        continue_dialogue = False\n",
        "        print(\"Agente Tupiniquim: Até a próxima.\")\n",
        "\n",
        "    # Se for agradecimento, eu também encerro\n",
        "    elif human_text in [\"obrigado\", \"muito obrigado\", \"agradecido\"]:\n",
        "        continue_dialogue = False\n",
        "        print(\"Agente Tupiniquim: Disponha\")\n",
        "\n",
        "    # Se for uma saudação, eu respondo de forma aleatória\n",
        "    elif geradorsaudacoes(human_text) is not None:\n",
        "        print(\"Agente Tupiniquim:\", geradorsaudacoes(human_text))\n",
        "\n",
        "    # Caso contrário, eu tento responder usando o corpus\n",
        "    else:\n",
        "        print(\"Agente Tupiniquim:\", geradorrespostas(human_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre o Brasil.\n",
            "Agente Tupiniquim: isto criou uma cozinha nacional marcada pela preservação das diferenças regionais.\n",
            "Agente Tupiniquim: o futebol é o esporte mais popular no brasil.\n",
            "Agente Tupiniquim: Me desculpe, não encontrei nada relevante sobre isso.\n",
            "Agente Tupiniquim: Me desculpe, não encontrei nada relevante sobre isso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMOce1PXp1h"
      },
      "source": [
        "### O que pode ser feito?\n",
        "Utilizamos um modelo baseado em regras, onde uma das regras faz uso de um corpus de dados para formular as respostas, o que já deixou nosso modelo bem mais flexível, sem necessidade de criação de centenas/milhares de regras.\n",
        "\n",
        "Mas, o que poderíamos fazer para melhorar?\n",
        "\n",
        "\n",
        "*   Poderíamos obter não apenas os parágrafos(`<p>`) na página da wikipedia, mas também utilizar os dados dispostos na coluna direita, que apresentam informações bem relevantes como população, atual presidente, etc., para montar sentenças.\n",
        "*   Poderíamos melhorar ainda mais o cálculo de similaridade ao utilizar um modelo de Word Embeddings, além do TF-IDF.\n",
        "*   Obter dados sobre o Brasil de diferentes fontes.\n",
        "*   Criar um classificador de contexto para o agente, e de maneira dinâmica buscar páginas da Wikipedia correspondentes à pergunta do usuário, e só então dar a resposta. Assim nosso agente não ficaria limitado a perguntas sobre o Brasil.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP59ECc53nqF"
      },
      "source": [
        "## Referências e Material complementar\n",
        "\n",
        "* [Python for NLP: Creating a Rule-Based Chatbot](https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/)\n",
        "* [Building a Simple Chatbot from Scratch in Python (using NLTK)](https://morioh.com/p/6cc33336784c)\n",
        "* [Building a simple chatbot in python](https://medium.com/nxtplus/building-a-simple-chatbot-in-python-3963618c490a)\n",
        "* [Designing A ChatBot Using Python: A Modified Approach](https://towardsdatascience.com/designing-a-chatbot-using-python-a-modified-approach-96f09fd89c6d)\n",
        "* [Build Your First Python Chatbot Project](https://dzone.com/articles/python-chatbot-project-build-your-first-python-pro)\n",
        "* [Python Chatbot Project – Learn to build your first chatbot using NLTK & Keras](https://data-flair.training/blogs/python-chatbot-project/)\n",
        "* [Python Chat Bot Tutorial - Chatbot with Deep Learning (Part 1)](https://www.youtube.com/watch?v=wypVcNIH6D4)\n",
        "* [Intelligent AI Chatbot in Python](https://www.youtube.com/watch?v=1lwddP0KUEg)\n",
        "* [Coding a Jarvis AI Using Python 3 For Beginners](https://www.youtube.com/watch?v=NZMTWBpLUa4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwExBlA63rt9"
      },
      "source": [
        "Este notebook foi produzido por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500)."
      ]
    }
  ]
}